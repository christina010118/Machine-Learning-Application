---
title: "HW1"
author: "Yiting Zhang (605325840)"
date: "4/8/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())

library(stats)
library(ISLR)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(GGally)
library(boot)
library(e1071)

```


# Problem 4.5
We now examine the differences between LDA and QDA.
(a) If the Bayes decision boundary is linear, do we expect LDA or
QDA to perform better on the training set? On the test set?

We expect QDA to perform better on the training set because its increased
flexibility will result in a better fit. If the Bayes decision boundary is 
linear, we expect LDA to perform better on the test set.

(b) If the Bayes decision boundary is non-linear, do we expect LDA
or QDA to perform better on the training set? On the test set?

We would expect QDA to perform better on both training and test set.

(c) In general, as the sample size n increases, do we expect the test
prediction accuracy of QDA relative to LDA to improve, decline,
or be unchanged? Why?

We expect the test prediction accuracy to improve as n gets bigger. In general, 
as the the sample size increases, a more flexible method will yield a better fit 
as the variance is offset by the larger sample size.

(d) True or False: Even if the Bayes decision boundary for a given
problem is linear, we will probably achieve a superior test error rate using 
QDA rather than LDA because QDA is flexible enough to model a linear decision 
boundary. Justify your answer.

False. It would results in overfitting if we use QDA on a few sample points, 
which could yield a higher test errors than LDA.

# Problem 4.12
Suppose that you wish to classify an observation X ∈ R into apples
and oranges.

(a) What is the log odds of orange versus apple in your model?
$\frac{P(x)}{1-P(x)}=e^{\beta_0+\beta_1X}$

log odds:
$log(\frac{P(x)}{1-P(x)})=\beta_0+\beta_1X$

(b) What is the log odds of orange versus apple in your friend’s
model?

log odds:
$log(\frac{Pr(Y=Orange|X=x)}{Pr(Y=Apple|X=x)}=\alpha_{orange0}-\alpha_{apple0}+(\alpha_{orange1}-\alpha_{apple1})X$


(c) Suppose that in your model, $\beta^0 = 2$ and $\beta^1 = -1$. What are
the coefficient estimates in your friend’s model? Be as specific
as possible.

The softmax coding is equivalent softmax to the coding just described in the sense that the fitted values, log odds between any pair of classes, and other key model outputs will remain the same, regardless of coding.
Therefore, $\alpha_{orange0}-\alpha_{apple0}=2$  and $\alpha_{orange1}-\alpha_{apple1}=-1$

(d) Now suppose that you and your friend fit the same two models
on a different data set. This time, your friend gets the coefficient
estimates $\alpha_{orange0}=1.2$,$\alpha_{orange1}=-2$, $\alpha_{apple0}=3$,$\alpha_{apple1}=0.6$. What are the coefficient estimates in your model?

$\beta^0 = 1.2-3=-1.8$ and $\beta^1 = -2-0.6=-2.6$ 

(e) Finally, suppose you apply both models from (d) to a data set
with 2,000 test observations. What fraction of the time do you
expect the predicted class labels from your model to agree with
those from your friend’s model? Explain your answer.

My predicted class labels will match with my friend's model because the key model outputs will remain the same for both cases. Softmax with more than two classes is logistic regression.


# Problem 4.14
In this problem, you will develop a model to predict whether a given
car gets high or low gas mileage based on the Auto data set.

```{r}

plot(Auto$mpg)


```

(a) Create a binary variable, mpg01, that contains a 1 if mpg contains
a value above its median, and a 0 if mpg contains a value below
its median. You can compute the median using the median()
function. Note you may find it helpful to use the data.frame()
function to create a single data set containing both mpg01 and
the other Auto variables.

```{r}

data('Auto')
Auto <- Auto %>%
    filter(!cylinders %in% c(3,5)) %>%
    mutate(mpg01 = factor(ifelse(mpg > median(mpg), 1, 0)),
           cylinders = factor(cylinders, 
                              levels = c(4,6,8),
                              ordered = TRUE),
           origin = factor(origin,
                           levels = c(1,2,3),
                           labels = c('American', 'European', 'Asian')))
median(Auto$mpg)


```


(b) Explore the data graphically in order to investigate the association between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.

```{r}


panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...) {
    usr <- par("usr")
    on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    Cor <- abs(cor(x, y)) # Remove abs function if desired
    txt <- paste0(prefix, format(c(Cor, 0.123456789), digits = digits)[1])
    if(missing(cex.cor)) {
        cex.cor <- 0.4 / strwidth(txt)
    }
    text(0.5, 0.5, txt,
         cex = 1 + cex.cor * Cor) # Resize the text by level of correlation
}

# Plotting the correlation matrix
pairs(Auto,
      upper.panel = panel.cor,    # Correlation panel
      lower.panel = panel.smooth, # Smoothed regression lines
      cex=0.8) 


```

```{r}

par(mfrow=c(2,3))
boxplot(cylinders ~ mpg01, data = Auto, main = "Cylinders vs mpg01")
boxplot(displacement ~ mpg01, data = Auto, main = "Displacement vs mpg01")
boxplot(horsepower ~ mpg01, data = Auto, main = "Horsepower vs mpg01")
boxplot(weight ~ mpg01, data = Auto, main = "Weight vs mpg01")
boxplot(acceleration ~ mpg01, data = Auto, main = "Acceleration vs mpg01")
boxplot(year ~ mpg01, data = Auto, main = "Year vs mpg01")

```

From the plots,cylinders,displacement,horsepower, and weight seem to be most associated with mpg01.

(c) Split the data into a training set and a test set.

```{r}

set.seed(12345678)
train <- sample(1:dim(Auto)[1], dim(Auto)[1]*.75, rep=FALSE)
test <- -train
training_Auto<- Auto[train, ]
testing_Auto= Auto[test, ]
mpg01.test <- Auto$mpg01[test]

```

(d) Perform LDA on the training data in order to predict mpg01
using the variables that seemed most associated with mpg01 in
(b). What is the test error of the model obtained?

```{r}

library(MASS)
lda.fit1=lda(mpg01~cylinders+displacement+horsepower+weight,
            data=training_Auto)
lda.fit1
plot(lda.fit1)
lda.class1=predict(lda.fit1,testing_Auto)$class
table(lda.class1,mpg01.test)
mean(lda.class1!= mpg01.test)


```
The test error rate is around 11.3%. 

(e) Perform QDA on the training data in order to predict mpg01
using the variables that seemed most associated with mpg01 in
(b). What is the test error of the model obtained?

```{r}

qda.fit1=qda(mpg01~cylinders+displacement+horsepower+weight,
            data=training_Auto)
qda.fit1
qda.class1=predict(qda.fit1,testing_Auto)$class
table(qda.class1,testing_Auto$mpg01)
mean(qda.class1!=testing_Auto$mpg01)

```
The test error rate is around 10.3%. 


(f) Perform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?

```{r}

glm.fit1=glm(mpg01~cylinders+displacement+horsepower+weight,family=binomial,
            data=training_Auto)
glm.prob1=predict(glm.fit1,testing_Auto,type="response")
pred.glm1 <- rep(0, length(glm.prob1))
pred.glm1[glm.prob1 > 0.5] <- 1
table(pred.glm1,testing_Auto$mpg01)
mean(pred.glm1!=testing_Auto$mpg01)

```
The test error rate is around 12.3%. 

(g) Perform naive Bayes on the training data in order to predict
mpg01 using the variables that seemed most associated with mpg01
in (b). What is the test error of the model obtained?

```{r}


# Create training and validation sets.
selected.var <- c(1, 2, 3, 4, 5)
train.index <- sample(c(1:dim(Auto)[1]), dim(Auto)[1]*0.75)  
train.Auto <- Auto[train.index, selected.var]
valid.Auto <- Auto[-train.index, selected.var]


Auto.nb <- naiveBayes(mpg01 ~ cylinders+displacement+horsepower+weight, data=training_Auto)


pred.prob <- predict(Auto.nb,testing_Auto, type = "raw")
pred.class <- predict(Auto.nb, testing_Auto)
table(pred.class, testing_Auto$mpg01 )
mean(pred.class !=testing_Auto$mpg01)



```
The error is 12.3%.

(h) Perform KNN on the training data, with several values of K, in
order to predict mpg01. Use only the variables that seemed most
associated with mpg01 in (b). What test errors do you obtain?
Which value of K seems to perform the best on this data set?

```{r}

library(class)

training_Auto1 = Auto[train,c("cylinders","displacement",  "horsepower","weight")]
testing_Auto1 = Auto[test, c("cylinders","displacement",  "horsepower","weight")]
mpg01.train <- Auto$mpg01[train]
mpg01.test <- Auto$mpg01[test]


set.seed(1)
knn.pred1=knn(training_Auto1,testing_Auto1,mpg01.train,k=1)
table(knn.pred1,mpg01.test)
mean(knn.pred1!=mpg01.test)

knn.pred2=knn(training_Auto1,testing_Auto1,mpg01.train,k=3)
table(knn.pred2,mpg01.test)
mean(knn.pred2!=mpg01.test)

```
Thw error rate is around 12%

# Problem 4.16

Using the Boston data set, fit classification models in order to predict
whether a given census tract has a crime rate above or below the median. Explore logistic regression, LDA, naive Bayes, and KNN models
using various subsets of the predictors. Describe your findings.
Hint: You will have to create the response variable yourself, using the
variables that are contained in the Boston data set.

```{r}

data("Boston")
Boston <- Boston %>%
    mutate(crim01 = factor(ifelse(crim > median(crim), 1, 0)))
median(Boston$crim)

```

```{r}

# Plotting the correlation matrix
pairs(Boston,
      upper.panel = panel.cor,    # Correlation panel
      lower.panel = panel.smooth, # Smoothed regression lines
      cex=0.8) 


```
Form the graph we can see that nox, indus, age, rad, and tax are mostly associated with crim01. These can be use as predictors later for crim01.

```{r}

#splitting the data into train and testing
set.seed(123456)
train <- sample(1:dim(Boston)[1], dim(Boston)[1]*.75, rep=FALSE)
test <- -train
Boston.train <- Boston[train, ]
Boston.test <- Boston[test, ]
crim01.test <- Boston$crim01[test]

```


```{r}
#LDA
lda.fit2=lda(crim01~nox+indus+age+rad+tax,
            data=Boston.train)
lda.fit2
plot(lda.fit2)
lda.class2=predict(lda.fit2,Boston.test)$class
table(lda.class2,crim01.test)
mean(lda.class2 != crim01.test)


```
The error rate is 14.9%.

```{r}

#QDA

qda.fit2=qda(crim01~nox+indus+age+rad+tax,
            data=Boston.train)
qda.fit2
qda.class2=predict(qda.fit2,Boston.test)$class
table(qda.class2,crim01.test)
mean(qda.class2!=crim01.test)

```
The error rate is 19.6%.


```{r}

#Logistic

glm.fit2=glm(crim01~nox+indus+age+rad+tax,
            data=Boston.train,family=binomial)
glm.prob2=predict(glm.fit2,Boston.test,type="response")
pred.glm2 <- rep(0, length(glm.prob2))
pred.glm2[glm.prob2 > 0.5] <- 1
table(pred.glm2,Boston.test$crim01)
mean(pred.glm2!=Boston.test$crim01)

```
The test error rate is around 18.1%. 

```{r}


#Naive Bayes

# Create training and validation sets.
selected.var2 <- c(1 ,3, 5, 7, 9, 10)
train.index2 <- sample(c(1:dim(Boston)[1]), dim(Boston)[1]*0.75)  
train.Boston <- Boston[train.index2, selected.var2]
valid.Boston <- Boston[-train.index2, selected.var2]

Boston.nb <- naiveBayes(crim01 ~ nox+indus+age+rad+tax, data=Boston.train)
summary(Boston.nb)


pred.prob2 <- predict(Boston.nb,Boston.test, type = "raw")
pred.class2 <- predict(Boston.nb, Boston.test)
table(pred.class2, Boston.test$crim01 )
mean(pred.class2 !=Boston.test$crim01)


```

The error rate from Naive Bayes is 20.4%.

```{r}

#KNN

library(class)
train.Boston1 = Boston[train,c("nox","indus","age","rad","tax")]
test.Boston1 = Boston[test, c("nox","indus","age","rad","tax")]
crim01.train <- Boston$crim01[train]
crim01.test <- Boston$crim01[test]

set.seed(123)
knn.pred3=knn(train.Boston1,test.Boston1,crim01.train,k=1)
table(knn.pred3,crim01.test)
mean(knn.pred3!=crim01.test)

knn.pred4=knn(train.Boston1,test.Boston1,crim01.train,k=3)
table(knn.pred4,crim01.test)
mean(knn.pred4!=crim01.test)

```
The error rate is around 9.44%.

# Problem 5.7

In Sections 5.3.2 and 5.3.3, we saw that the cv.glm() function can be
used in order to compute the LOOCV test error estimate. Alternatively, one could compute those quantities using just the glm() and predict.glm() functions, and a for loop. You will now take this approach in order to compute the LOOCV error for a simple logistic
regression model on the Weekly data set. Recall that in the context
of classification problems, the LOOCV error is given in (5.4).

(a) Fit a logistic regression model that predicts Direction using Lag1
and Lag2.

```{r}

data('Weekly')

fit.glm3 = glm(Direction ~ Lag1 + Lag2, data= Weekly, family = binomial)
summary(fit.glm3)

```

(b) Fit a logistic regression model that predicts Direction using Lag1
and Lag2 using all but the first observation.

```{r}

fit.glm4 = glm(Direction ~ Lag1 + Lag2, data= Weekly[-1,], family = binomial)
summary(fit.glm4)

```

(c) Use the model from (b) to predict the direction of the first observation. You can do this by predicting that the first observation will go up if $P(Direction = "Up"|Lag1, Lag2) > 0.5$. Was this observation correctly classified?

```{r}

predict(fit.glm4, newdata = Weekly[1,], type = "response") > 0.5
Weekly[1,]$Direction

```
The prediction said the first observation is up, but this is not correctly classified because the true direction is down.

(d) Write a for loop from i = 1 to i = n, where n is the number of
observations in the data set, that performs each of the following
steps:
i. Fit a logistic regression model using all but the ith observation to predict Direction using Lag1 and Lag2.
ii. Compute the posterior probability of the market moving up
for the ith observation.
iii. Use the posterior probability for the ith observation in order
to predict whether or not the market moves up.
iv. Determine whether or not an error was made in predicting
the direction for the ith observation. If an error was made,
then indicate this as a 1, and otherwise indicate it as a 0.

```{r}

observation <- dim(Weekly)[1]

error <- rep(0, observation) # set error =0
for (i in 1:dim(Weekly)[1]) {
    fit.glm <- glm(Direction ~ Lag1 + Lag2, data = Weekly[-i, ],  family = "binomial")
    #i. Fit a logistic regression model using all but the ith observation to predict Direction using Lag1 and Lag2.
    pred.up <- predict.glm(fit.glm, Weekly[i, ], type = "response") > 0.5
    #ii. Compute the posterior probability of the market moving up for the ith observation.
    #iii. Use the posterior probability for the ith observation in order to predict whether or not the market moves up.
    true.up <- Weekly[i, ]$Direction == "Up"
    if (pred.up != true.up)
        error[i] <- 1
    #iv. Determine whether or not an error was made in predicting the direction for the ith observation. If an error was made,then indicate this as a 1, and otherwise indicate it as a 0.
}
error

```

(e) Take the average of the n numbers obtained in (d)iv in order to
obtain the LOOCV estimate for the test error. Comment on the
results.

Average out to get the test error

```{r}

mean(error)

```

# Problem 5.9

We will now consider the Boston housing data set, from the ISLR2
library.

(a) Based on this data set, provide an estimate for the population
mean of medv. Call this estimate $\hat\mu$.

```{r}

mu <- mean(Boston$medv)
mu

```

(b) Provide an estimate of the standard error of $\hat\mu$. Interpret this
result.
Hint: We can compute the standard error of the sample mean by
dividing the sample standard deviation by the square root of the
number of observations.

```{r}

obs <- dim(Boston)[1]
se <- sd(Boston$medv) /sqrt(obs)
se

```

The standard error is 0.4088

(c) Now estimate the standard error of $\hat\mu$ using the bootstrap. How
does this compare to your answer from (b)?

```{r}

set.seed(123)
boot.func<-function(data,index){
  muu<-mean(data[index])
  return(muu)
}
set.seed(123)
boot(Boston$medv,boot.func,99)

```

The standard error obtained form bootstrap is 0.4415. Similar to what we have from part (b).

(d) Based on your bootstrap estimate from (c), provide a 95 % confidence interval for the mean of medv. Compare it to the results obtained using t.test(Boston$medv).
Hint: You can approximate a 95 % confidence interval using the
formula $[\hat\mu − 2SE(\hat\mu), \hat\mu + 2SE(\hat\mu)]$.

```{r}

mu_ci<-c(mu-2*0.4414568,mu+2*0.4414568)
mu_ci
# CI= (21.64989, 23.41572)

t.test(Boston$medv)
# CI= (21.72953, 23.33608)

```

The confidence intervals are pretty similar.

(e) Based on this data set, provide an estimate, $\hat\mu_{med}$, for the median
value of medv in the population.

```{r}

med <- median(Boston$medv)
med

```

(f) We now would like to estimate the standard error of  $\hat\mu_{med}$. Unfortunately, there is no simple formula for computing the standard error of the median. Instead, estimate the standard error of the median using the bootstrap. Comment on your findings.

```{r}

boot.func <- function(data, index) {
    muu <- median(data[index])
    return (muu)
}
set.seed(123)
boot(Boston$medv, boot.func, 999)

```

We get the same median value of 21.2 as the one in part (e), with a standard error of 0.3727. It also shows how easy it is to use bootstrap to calculate error rate.

(g) Based on this data set, provide an estimate for the tenth percentile of medv in Boston census tracts. Call this quantity $\hat\mu_{0.1}$. (You can use the quantile() function.)

```{r}

medv_0.1 <- quantile(Boston$medv, c(0.1))
medv_0.1 

```
The estimate for the tenth percentile of medv in Boston suburbs is 12.75.

(h) Use the bootstrap to estimate the standard error of $\hat\mu_{0.1}$. Comment on your findings.

```{r}

boot.func <- function(data, index) {
  mu_0.1 <- quantile(data[index], c(0.1))
  return (mu_0.1)
}
set.seed(123)
boot(Boston$medv, boot.func, 999)

```

The standard error from bootstrap is 0.5036, and we got the same value for median was in part (g).